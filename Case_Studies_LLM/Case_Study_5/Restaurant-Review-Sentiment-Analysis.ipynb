{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lQMN7B5FKR2S",
   "metadata": {
    "id": "lQMN7B5FKR2S"
   },
   "source": [
    "\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Italiana&display=swap\" rel=\"stylesheet\">\n",
    "\n",
    "<center>\n",
    "  <span style=\"font-family:'Times New Roman', serif; font-size:48px;\">\n",
    "    Introduction to Prompt Engineering:\n",
    "  </span>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "  <span style=\"font-family:'Italiana', serif; font-size:48px; font-style:italic;\">\n",
    "    Restaurant Review Analysis\n",
    "  </span>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G-jLkpBYdP89",
   "metadata": {
    "id": "G-jLkpBYdP89"
   },
   "source": [
    "<font face=\"Times New Roman\" size=6> Problem Statement</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xClk0SOxdU_s",
   "metadata": {
    "id": "xClk0SOxdU_s"
   },
   "source": [
    "### Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cPfUxkrbQpmc",
   "metadata": {
    "id": "cPfUxkrbQpmc"
   },
   "source": [
    "The manual evaluation of unstructured customer reviews is time-consuming and impractical for large datasets. The company faces challenges in automatically identifying and classifying customer sentiments (positive, negative, or neutral) from textual feedback, hindering data-driven decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MzSKXh2LsOvd",
   "metadata": {
    "id": "MzSKXh2LsOvd"
   },
   "source": [
    "### Business Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "my--X4VNQsPd",
   "metadata": {
    "id": "my--X4VNQsPd"
   },
   "source": [
    "The company receives a vast number of customer reviews across its restaurant outlets. These reviews hold valuable insights that can help improve service quality, customer satisfaction, and marketing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mgGZFBqvdX3_",
   "metadata": {
    "id": "mgGZFBqvdX3_"
   },
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5Q0UjpRLUteW",
   "metadata": {
    "id": "5Q0UjpRLUteW"
   },
   "source": [
    "Develop an automated sentiment analysis system powered by a Large Language Model (LLM) to accurately predict customer sentiments and generate actionable insights for enhancing customer satisfaction and business decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8pDl8uVKR2W",
   "metadata": {
    "id": "b8pDl8uVKR2W"
   },
   "source": [
    "## Installing and Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fN9ATQxwKR2W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 180058,
     "status": "ok",
     "timestamp": 1760344701240,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "fN9ATQxwKR2W",
    "outputId": "76c3c749-4b87-459e-cca2-0fe76816657a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.7/36.7 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m327.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m280.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m359.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m321.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m211.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.3 which is incompatible.\n",
      "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.3 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.3 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.45 --force-reinstall --no-cache-dir -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VFjHLA6SKR2W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9262,
     "status": "ok",
     "timestamp": 1760344710505,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "VFjHLA6SKR2W",
    "outputId": "7676ddf2-2e38-476f-8a2b-0a041c5ca77e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.35.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mOyQTXjGVZzW",
   "metadata": {
    "id": "mOyQTXjGVZzW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mv_4wV7dTNqQ",
   "metadata": {
    "id": "mv_4wV7dTNqQ"
   },
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b98a2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zb2dDd4ekE_V",
   "metadata": {
    "executionInfo": {
     "elapsed": 2004,
     "status": "ok",
     "timestamp": 1760595413251,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "zb2dDd4ekE_V"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"D:\\5th sem\\LLM\\restaurant_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H-51wMmpGzcr",
   "metadata": {
    "id": "H-51wMmpGzcr"
   },
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "duHgKeWUqlzQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1760595499400,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "duHgKeWUqlzQ",
    "outputId": "efc33a17-75fd-4dd1-8029-87ac38c6d024"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We came across Perch by accident and had dinner there on two separate occasions, having enjoyed it so much the first time round. There is no doubt that it has a strong European emphasis, although results in fantastic, modern food and attentive service that was amongst the best that we've experienced here.  The Indian Sauvignon Blanc was very acceptable, rather than the more expensive European alternatives, and there was a good range of cocktails, mocktails and beers. Decor was stylish but simple, ambience was good with lively, tasteful music to suit the meal. All food we ordered was good, and we've taken a menu home to replicate some of our favourite dishes. Have two appetisers or one main each, a couple of puddings are good to share on a small table.  Half carafe of wine, half of Sangria, two beers, five appetisers and two puddings including taxes and service about £75. \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review_full'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "NA2wS2mVkK7X",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1760344738826,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "NA2wS2mVkK7X",
    "outputId": "d3036b34-afd9-44ae-fcee-f600789fb6d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant_ID</th>\n",
       "      <th>rating_review</th>\n",
       "      <th>review_full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLV202</td>\n",
       "      <td>5</td>\n",
       "      <td>Totally in love with the Auro of the place, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAV303</td>\n",
       "      <td>5</td>\n",
       "      <td>Kailash colony is brimming with small cafes no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YUM789</td>\n",
       "      <td>5</td>\n",
       "      <td>Excellent taste and awesome decorum. Must visi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TST101</td>\n",
       "      <td>5</td>\n",
       "      <td>I have visited at jw lough/restourant. There w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EAT456</td>\n",
       "      <td>5</td>\n",
       "      <td>Had a great experience in the restaurant food ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  restaurant_ID  rating_review  \\\n",
       "0        FLV202              5   \n",
       "1        SAV303              5   \n",
       "2        YUM789              5   \n",
       "3        TST101              5   \n",
       "4        EAT456              5   \n",
       "\n",
       "                                         review_full  \n",
       "0  Totally in love with the Auro of the place, re...  \n",
       "1  Kailash colony is brimming with small cafes no...  \n",
       "2  Excellent taste and awesome decorum. Must visi...  \n",
       "3  I have visited at jw lough/restourant. There w...  \n",
       "4  Had a great experience in the restaurant food ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "m0E69LFPThSJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1760344738877,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "m0E69LFPThSJ",
    "outputId": "12378cac-6c2d-462b-c1d8-1744ec6a007a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "udY09oFqTm1B",
   "metadata": {
    "id": "udY09oFqTm1B"
   },
   "source": [
    "Data has 20 rows and 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "qMV8hC_2G9aW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1760344738885,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "qMV8hC_2G9aW",
    "outputId": "369f827d-14e6-45d9-e244-ed798a80d6b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "restaurant_ID    0\n",
       "rating_review    0\n",
       "review_full      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8zETWVmeHBIk",
   "metadata": {
    "id": "8zETWVmeHBIk"
   },
   "source": [
    "There are no missing values in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L5pIwimGK5_1",
   "metadata": {
    "id": "L5pIwimGK5_1"
   },
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jbaduRVymY3v",
   "metadata": {
    "id": "jbaduRVymY3v"
   },
   "source": [
    "### Loading the model (Llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61N9i3i8K5_9",
   "metadata": {
    "id": "61N9i3i8K5_9"
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\n",
    "model_basename = \"llama-2-13b-chat.Q5_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ZpjCaMRVcM-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "executionInfo": {
     "elapsed": 229479,
     "status": "ok",
     "timestamp": 1760344968411,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "0ZpjCaMRVcM-",
    "outputId": "dccd3a76-b835-4360-c044-b8b61dda028b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c69d9060fe48b59e6eb2f169a3fecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama-2-13b-chat.Q5_K_M.gguf:   0%|          | 0.00/9.23G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1SolgBhkVdgc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40564,
     "status": "ok",
     "timestamp": 1760345008977,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "1SolgBhkVdgc",
    "outputId": "1ded5dc7-580c-4554-decc-b79c268d5849"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGUF/snapshots/4458acc949de0a9914c3eab623904d4fe999050a/llama-2-13b-chat.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q5_K:  241 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 8.60 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  8801.63 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =  3200.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3200.00 MiB, K (f16): 1600.00 MiB, V (f16): 1600.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    19.04 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   360.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n"
     ]
    }
   ],
   "source": [
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=2,  \n",
    "    n_batch=512,  \n",
    "    n_ctx=4096, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oWvf3R3An5K4",
   "metadata": {
    "id": "oWvf3R3An5K4"
   },
   "source": [
    "### Loading the model (Mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rF2F_YO_qGtV",
   "metadata": {
    "id": "rF2F_YO_qGtV"
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
    "model_basename = \"mistral-7b-instruct-v0.2.Q6_K.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Uk2q7vrc_TrO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "Uk2q7vrc_TrO",
    "outputId": "e14f4275-3f05-4a06-95b9-74e3026b0a83"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11fc5810596486cad1452b15c36fc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral-7b-instruct-v0.2.Q6_K.gguf:   0%|          | 0.00/5.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = hf_hub_download(\n",
    "    repo_id=model_name_or_path,\n",
    "    filename=model_basename\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wI_T-0DWXRtD",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "wI_T-0DWXRtD",
    "outputId": "b5ab1dbe-1740-4b64-f0b6-642a39f843b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/mistral-7b-instruct-v0.2.Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 5.53 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  5666.09 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   128.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    11.01 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    96.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '18'}\n",
      "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: \n",
      "Using chat bos_token: \n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=1024,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VVXK7vYfmdkL",
   "metadata": {
    "id": "VVXK7vYfmdkL"
   },
   "source": [
    "### Defining Model Response Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5AfP0mcXVfXV",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5AfP0mcXVfXV"
   },
   "outputs": [],
   "source": [
    "def generate_llama_response(instruction, review):\n",
    "    system_message = \"\"\"\n",
    "        [INST]<<SYS>>\n",
    "        {}\n",
    "        <</SYS>>[/INST]\n",
    "    \"\"\".format(instruction)\n",
    "    prompt = f\"{review}\\n{system_message}\"\n",
    "    response = lcpp_llm(\n",
    "        prompt=prompt,\n",
    "        max_tokens=1024,\n",
    "        temperature=0,\n",
    "        top_p=0.95,\n",
    "        repeat_penalty=1.2,\n",
    "        top_k=50,\n",
    "        stop=['INST'],\n",
    "        echo=False,\n",
    "        seed=42,\n",
    "    )\n",
    "    response_text = response[\"choices\"][0][\"text\"]\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SMXvOtlrmHJC",
   "metadata": {
    "id": "SMXvOtlrmHJC"
   },
   "source": [
    "- **`max_tokens`**: This parameter **specifies the maximum number of tokens that the model should generate** in response to the prompt.\n",
    "\n",
    "- **`temperature`**: This parameter **controls the randomness of the generated response**. A higher temperature value will result in a more random response, while a lower temperature value will result in a more predictable response.\n",
    "\n",
    "- **`top_p`**: This parameter **controls the diversity of the generated response by establishing a cumulative probability cutoff for token selection**. A higher value of top_p will result in a more diverse response, while a lower value will result in a less diverse response.\n",
    "\n",
    "- **`repeat_penalty`**: This parameter **controls the penalty for repeating tokens in the generated response**. A higher value of repeat_penalty will result in a lower probability of repeating tokens, while a lower value will result in a higher probability of repeating tokens.\n",
    "\n",
    "- **`top_k`**: This parameter **controls the maximum number of most-likely next tokens to consider** when generating the response at each step.\n",
    "\n",
    "- **`stop`**: This parameter is a **list of tokens that are used to dynamically stop response generation** whenever the tokens in the list are encountered.\n",
    "\n",
    "- **`echo`**: This parameter **controls whether the input (prompt) to the model should be returned** in the model response.\n",
    "\n",
    "- **`seed`**: This parameter **specifies a seed value that helps replicate results**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CHwEJ-hYjZyw",
   "metadata": {
    "id": "CHwEJ-hYjZyw"
   },
   "source": [
    "### Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OWcEelJkO0uq",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "OWcEelJkO0uq"
   },
   "outputs": [],
   "source": [
    "def extract_json_data(json_str):\n",
    "    try:\n",
    "        json_start = json_str.find('{')\n",
    "        json_end = json_str.rfind('}')\n",
    "\n",
    "        if json_start != -1 and json_end != -1:\n",
    "            extracted_sentiment = json_str[json_start:json_end + 1]  \n",
    "            data_dict = json.loads(extracted_sentiment)\n",
    "            return data_dict\n",
    "        else:\n",
    "            print(f\"Warning: JSON object not found in response: {json_str}\")\n",
    "            return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BhARt3BUmHJA",
   "metadata": {
    "id": "BhARt3BUmHJA"
   },
   "source": [
    "## 1. Sentiment Analysis (Llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ifE2NALEmysV",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ifE2NALEmysV"
   },
   "outputs": [],
   "source": [
    "data_1 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NaOpXQfumHJC",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "NaOpXQfumHJC"
   },
   "outputs": [],
   "source": [
    "instruction_1 = \"\"\"\n",
    "    You are an AI analyzing restaurant reviews. Classify the sentiment of the provided review into the following categories:\n",
    "    - Positive\n",
    "    - Negative\n",
    "    - Neutral\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bead3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1['model_response'] = data_1['review_full'].apply(lambda x: generate_llama_response(instruction_1, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76Ym659-m7yI",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "76Ym659-m7yI"
   },
   "outputs": [],
   "source": [
    "data_1['model_response'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832fdf50",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "832fdf50"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iQEDE21ymHJD",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "iQEDE21ymHJD"
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "print(data_1.loc[i, 'review_full'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Dzuc2gsu4gCZ",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Dzuc2gsu4gCZ"
   },
   "outputs": [],
   "source": [
    "print(data_1.loc[i, 'model_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_s0tz3CmmHJD",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_s0tz3CmmHJD"
   },
   "outputs": [],
   "source": [
    "def extract_sentiment(model_response):\n",
    "    if 'positive' in model_response.lower():\n",
    "        return 'Positive'\n",
    "    elif 'negative' in model_response.lower():\n",
    "        return 'Negative'\n",
    "    elif 'neutral' in model_response.lower():\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_ie4vWp_mHJE",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_ie4vWp_mHJE"
   },
   "outputs": [],
   "source": [
    "data_1['sentiment'] = data_1['model_response'].apply(extract_sentiment)\n",
    "data_1['sentiment'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wdx85PqRf1H7",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Wdx85PqRf1H7"
   },
   "outputs": [],
   "source": [
    "data_1['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BAajLpLOmHJF",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "BAajLpLOmHJF"
   },
   "outputs": [],
   "source": [
    "final_data_1 = data_1.drop(['model_response'], axis=1)\n",
    "final_data_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sq1Aq8BXajfg",
   "metadata": {
    "id": "sq1Aq8BXajfg"
   },
   "source": [
    "### 1. Sentiment Analysis from Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ScqJfjzyajgD",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ScqJfjzyajgD"
   },
   "outputs": [],
   "source": [
    "data_1 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o1NHOv7fa5LU",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "o1NHOv7fa5LU"
   },
   "outputs": [],
   "source": [
    "def response_1(prompt,review):\n",
    "    model_output = llm(\n",
    "      f\"\"\"\n",
    "      Q: {prompt}\n",
    "      Review: {review}\n",
    "      A:\n",
    "      \"\"\",\n",
    "      max_tokens=32,\n",
    "      stop=[\"Q:\", \"\\n\"],\n",
    "      temperature=0.01,\n",
    "      echo=False,\n",
    "    )\n",
    "\n",
    "    temp_output = model_output[\"choices\"][0][\"text\"]\n",
    "\n",
    "    return temp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kOCUHdAiajgE",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "kOCUHdAiajgE"
   },
   "outputs": [],
   "source": [
    "instruction_1 = \"\"\"\n",
    "    You are an AI analyzing restaurant reviews. Classify the sentiment of the provided review into the following categories:\n",
    "    - Positive\n",
    "    - Negative\n",
    "    - Neutral\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m4c3-lWwajgF",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "m4c3-lWwajgF"
   },
   "outputs": [],
   "source": [
    "data_1['model_response'] = data_1['review_full'].apply(lambda x: response_1(instruction_1, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZD036AJSajgG",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ZD036AJSajgG"
   },
   "outputs": [],
   "source": [
    "data_1['model_response'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B4xz7jZmajgH",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "B4xz7jZmajgH"
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "print(data_1.loc[i, 'review_full'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fsAehjb6ajgI",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "fsAehjb6ajgI"
   },
   "outputs": [],
   "source": [
    "print(data_1.loc[i, 'model_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bckp7sqXajgI",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Bckp7sqXajgI"
   },
   "outputs": [],
   "source": [
    "def extract_sentiment(model_response):\n",
    "    if 'positive' in model_response.lower():\n",
    "        return 'Positive'\n",
    "    elif 'negative' in model_response.lower():\n",
    "        return 'Negative'\n",
    "    elif 'neutral' in model_response.lower():\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RzZQ9ZZTajgJ",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "RzZQ9ZZTajgJ"
   },
   "outputs": [],
   "source": [
    "data_1['sentiment'] = data_1['model_response'].apply(extract_sentiment)\n",
    "data_1['sentiment'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t4yh3oF1ajgK",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "t4yh3oF1ajgK"
   },
   "outputs": [],
   "source": [
    "data_1['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2mgE55M2ajgL",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "2mgE55M2ajgL"
   },
   "outputs": [],
   "source": [
    "final_data_1 = data_1.drop(['model_response'], axis=1)\n",
    "final_data_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ixmkBFZjh3Uu",
   "metadata": {
    "id": "ixmkBFZjh3Uu"
   },
   "source": [
    "### 2. Sentiment Analysis and Returning Structured Output from Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mgee5PGLnhNr",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Mgee5PGLnhNr"
   },
   "outputs": [],
   "source": [
    "data_2 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kjDHDEs0cX5U",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "kjDHDEs0cX5U"
   },
   "outputs": [],
   "source": [
    "instruction_2 = \"\"\"\n",
    "    You are an AI analyzing restaurant reviews. Classify the sentiment of the provided review into the following categories:\n",
    "    - Positive\n",
    "    - Negative\n",
    "    - Neutral\n",
    "\n",
    "    Format the output as a JSON object with a single key-value pair as shown below:\n",
    "    {\"sentiment\": \"your_sentiment_prediction\"}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SqtxeTOnh3VE",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "SqtxeTOnh3VE"
   },
   "outputs": [],
   "source": [
    "data_2['model_response'] = data_2['review_full'].apply(lambda x: generate_llama_response(instruction_2, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P2Zf6SsN4bsJ",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "P2Zf6SsN4bsJ"
   },
   "outputs": [],
   "source": [
    "data_2['model_response'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Sj9Kw8LLh3VF",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Sj9Kw8LLh3VF"
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "print(data_2.loc[i, 'review_full'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bxv3poEE4n3n",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Bxv3poEE4n3n"
   },
   "outputs": [],
   "source": [
    "print(data_2.loc[i, 'model_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A6Zo_YDch3VG",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "A6Zo_YDch3VG"
   },
   "outputs": [],
   "source": [
    "data_2['model_response_parsed'] = data_2['model_response'].apply(extract_json_data)\n",
    "data_2['model_response_parsed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qFPR8vPAh3VH",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "qFPR8vPAh3VH"
   },
   "outputs": [],
   "source": [
    "model_response_parsed_df_2 = pd.json_normalize(data_2['model_response_parsed'])\n",
    "model_response_parsed_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NX-zy5BTh3VH",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "NX-zy5BTh3VH"
   },
   "outputs": [],
   "source": [
    "data_with_parsed_model_output_2 = pd.concat([data_2, model_response_parsed_df_2], axis=1)\n",
    "data_with_parsed_model_output_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6hiEw_Znh3VI",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6hiEw_Znh3VI"
   },
   "outputs": [],
   "source": [
    "final_data_2 = data_with_parsed_model_output_2.drop(['model_response','model_response_parsed'], axis=1)\n",
    "final_data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xuZttnsMos6g",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "xuZttnsMos6g"
   },
   "outputs": [],
   "source": [
    "final_data_2['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HslGURoah3VI",
   "metadata": {
    "id": "HslGURoah3VI"
   },
   "source": [
    "### 3. Analyzing Overall and Aspect-Based Sentiments of Customer Experience Using Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MZcG8ygIn2qy",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MZcG8ygIn2qy"
   },
   "outputs": [],
   "source": [
    "data_3 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YhdIDYq4lf3C",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "YhdIDYq4lf3C"
   },
   "outputs": [],
   "source": [
    "instruction_3 = \"\"\"\n",
    "    You are an AI analyzing restaurant reviews. Classify the overall sentiment of the provided review into the following categories:\n",
    "    - \"Positive\"\n",
    "    - \"Negative\"\n",
    "    - \"Neutral\"\n",
    "\n",
    "    Once that is done, check for a mention of the following aspects in the review and classify the sentiment of each aspect as \"Positive\", \"Negative\", or \"Neutral\":\n",
    "    1. \"Food Quality\"\n",
    "    2. \"Service\"\n",
    "    3. \"Ambience\"\n",
    "\n",
    "    Output the overall sentiment and sentiment for each category in a JSON format with the following keys:\n",
    "    {\n",
    "        \"Overall\": \"your_sentiment_prediction\",\n",
    "        \"Food Quality\": \"your_sentiment_prediction\",\n",
    "        \"Service\": \"your_sentiment_prediction\",\n",
    "        \"Ambience\": \"your_sentiment_prediction\"\n",
    "    }\n",
    "\n",
    "    In case one of the three aspects is not mentioned in the review, set \"Not Applicable\" (including quotes) for the corresponding JSON key value.\n",
    "    Only return the JSON, do not return any other information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vOmka93ilf3O",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vOmka93ilf3O"
   },
   "outputs": [],
   "source": [
    "data_3['model_response'] = data_3['review_full'].apply(lambda x: generate_llama_response(instruction_3, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IwAEYkStbUjv",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "IwAEYkStbUjv"
   },
   "outputs": [],
   "source": [
    "data_3['model_response'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FnfnhInbup2A",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "FnfnhInbup2A"
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "print(data_3.loc[i, 'review_full'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tAqbJ5Hs4v-A",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "tAqbJ5Hs4v-A"
   },
   "outputs": [],
   "source": [
    "print(data_3.loc[i, 'model_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1t7hUgimunU5",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "1t7hUgimunU5"
   },
   "outputs": [],
   "source": [
    "data_3['model_response_parsed'] = data_3['model_response'].apply(extract_json_data)\n",
    "data_3['model_response_parsed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sX_m53eLunVA",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "sX_m53eLunVA"
   },
   "outputs": [],
   "source": [
    "model_response_parsed_df_3 = pd.json_normalize(data_3['model_response_parsed'])\n",
    "model_response_parsed_df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B37cFr6punVB",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "B37cFr6punVB"
   },
   "outputs": [],
   "source": [
    "data_with_parsed_model_output_3 = pd.concat([data_3, model_response_parsed_df_3], axis=1)\n",
    "data_with_parsed_model_output_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kMDwOWVPunVB",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "kMDwOWVPunVB"
   },
   "outputs": [],
   "source": [
    "final_data_3 = data_with_parsed_model_output_3.drop(['model_response','model_response_parsed'], axis=1)\n",
    "final_data_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gkyB71hC22kS",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "gkyB71hC22kS"
   },
   "outputs": [],
   "source": [
    "final_data_3['Overall'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uQ__aM1Hcy00",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "uQ__aM1Hcy00"
   },
   "outputs": [],
   "source": [
    "final_data_3['Food Quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ah3IRNDTcyxO",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Ah3IRNDTcyxO"
   },
   "outputs": [],
   "source": [
    "final_data_3['Service'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hPQZOYwicyui",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "hPQZOYwicyui"
   },
   "outputs": [],
   "source": [
    "final_data_3['Ambience'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Mf4lta2PbHS4",
   "metadata": {
    "id": "Mf4lta2PbHS4"
   },
   "source": [
    "### 4. Analyzing Overall and Aspect-Based Sentiments of Customer Experience Using Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZWwS7SIpbHTb",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ZWwS7SIpbHTb"
   },
   "outputs": [],
   "source": [
    "# creating a copy of the data\n",
    "data_3 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6N4lVvxkbz7Y",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6N4lVvxkbz7Y"
   },
   "outputs": [],
   "source": [
    "def response_2(prompt,review,sentiment):\n",
    "    model_output = llm(\n",
    "      f\"\"\"\n",
    "      Q: {prompt}\n",
    "      review: {review}\n",
    "      sentiment: {sentiment}\n",
    "      A:\n",
    "      \"\"\",\n",
    "      max_tokens=64,\n",
    "      stop=[\"Q:\", \"\\n\"],\n",
    "      temperature=0.01,\n",
    "      echo=False,\n",
    "    )\n",
    "\n",
    "    temp_output = model_output[\"choices\"][0][\"text\"]\n",
    "    final_output = temp_output[temp_output.index('{'):]\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "us5pXLe30NaB",
   "metadata": {
    "id": "us5pXLe30NaB"
   },
   "source": [
    "Note: Since the sentiment for each review has already been predicted in Task 1, we can utilize this information when designing the prompt for this task. This approach helps minimize redundant processing and reduces computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vmP6pXJQbHTc",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vmP6pXJQbHTc"
   },
   "outputs": [],
   "source": [
    "instruction_3 = \"\"\"\n",
    "    You are provided a review and it's sentiment.\n",
    "\n",
    "    Instructions:\n",
    "    Classify the sentiment of each aspect as either of \"Positive\", \"Negative\", or \"Neutral\" only and not any other for the given review:\n",
    "    1. \"Food Quality\"\n",
    "    2. \"Service\"\n",
    "    3. \"Ambience\"\n",
    "    In case one of the three aspects is not mentioned in the review, return \"Not Applicable\" (including quotes) for the corresponding JSON key value.\n",
    "    Return the output in the format {\"Overall\": given sentiment input,\"Food Quality\": \"your_sentiment_prediction\",\"Service\": \"your_sentiment_prediction\",\"Ambience\": \"your_sentiment_prediction\"}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mBQruWc9bHTd",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "mBQruWc9bHTd"
   },
   "outputs": [],
   "source": [
    "data_3['model_response'] = final_data_1[['review_full','sentiment']].apply(lambda x: response_2(instruction_3, x[0],x[1]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CvjebhnZbHTe",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "CvjebhnZbHTe"
   },
   "outputs": [],
   "source": [
    "data_3['model_response'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r-t5e6gMbHTf",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "r-t5e6gMbHTf"
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "print(data_3.loc[i, 'review_full'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h9yyFpQHbHTg",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "h9yyFpQHbHTg"
   },
   "outputs": [],
   "source": [
    "print(data_3.loc[i, 'model_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MuvmCV_DbHTh",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MuvmCV_DbHTh"
   },
   "outputs": [],
   "source": [
    "data_3['model_response_parsed'] = data_3['model_response'].apply(extract_json_data)\n",
    "data_3['model_response_parsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D2T6_J8vbHTi",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "D2T6_J8vbHTi"
   },
   "outputs": [],
   "source": [
    "model_response_parsed_df_3 = pd.json_normalize(data_3['model_response_parsed'])\n",
    "model_response_parsed_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F_KjQ3sXqAEj",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "F_KjQ3sXqAEj"
   },
   "outputs": [],
   "source": [
    "model_response_parsed_df_3 = model_response_parsed_df_3.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fudlsBnbHTj",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5fudlsBnbHTj"
   },
   "outputs": [],
   "source": [
    "data_with_parsed_model_output_3 = pd.concat([data_3, model_response_parsed_df_3], axis=1)\n",
    "data_with_parsed_model_output_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v6Ktnid1bHTk",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "v6Ktnid1bHTk"
   },
   "outputs": [],
   "source": [
    "final_data_3 = data_with_parsed_model_output_3.drop(['model_response','model_response_parsed'], axis=1)\n",
    "final_data_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dm8LimPbHTl",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "8dm8LimPbHTl"
   },
   "outputs": [],
   "source": [
    "final_data_3['Overall'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79rgJ_pObHTm",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "79rgJ_pObHTm"
   },
   "outputs": [],
   "source": [
    "final_data_3['Food Quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-a8rLEt3bHTm",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-a8rLEt3bHTm"
   },
   "outputs": [],
   "source": [
    "final_data_3['Service'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-JCcAI2UbHTn",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-JCcAI2UbHTn"
   },
   "outputs": [],
   "source": [
    "final_data_3['Ambience'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RxpHCNQeh3VL",
   "metadata": {
    "id": "RxpHCNQeh3VL"
   },
   "source": [
    "### 5. Identifying Overall and Aspect-Based Sentiments, and Extracting Liked or Disliked Features Using Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VtLikOc_vRsd",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "VtLikOc_vRsd"
   },
   "outputs": [],
   "source": [
    "data_4 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iJVySI-LcX3X",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "iJVySI-LcX3X"
   },
   "outputs": [],
   "source": [
    "instruction_4 = \"\"\"\n",
    "    You are an AI tasked with analyzing restaurant reviews. Your goal is to classify the overall sentiment of the provided review into the following categories:\n",
    "        - Positive\n",
    "        - Negative\n",
    "        - Neutral\n",
    "\n",
    "    Subsequently, assess the sentiment of specific aspects mentioned in the review, namely:\n",
    "        1. Food quality\n",
    "        2. Service\n",
    "        3. Ambience\n",
    "\n",
    "    Further, identify liked and/or disliked features associated with each aspect in the review.\n",
    "\n",
    "    Return the output in the specified JSON format, ensuring consistency and handling missing values appropriately:\n",
    "\n",
    "    {\n",
    "        \"Overall\": \"your_sentiment_prediction\",\n",
    "        \"Food Quality\": \"your_sentiment_prediction\",\n",
    "        \"Service\": \"your_sentiment_prediction\",\n",
    "        \"Ambience\": \"your_sentiment_prediction\",\n",
    "        \"Food Quality Features\": [\"liked/disliked features\"],\n",
    "        \"Service Features\": [\"liked/disliked features\"],\n",
    "        \"Ambience Features\": [\"liked/disliked features\"]\n",
    "    }\n",
    "\n",
    "    The sentiment prediction for Overall, Food Quality, Service, and Ambience should be one of \"Positive\", \"Negative\", or \"Neutral\" only.\n",
    "    In case one of the three aspects is not mentioned in the review, set \"Not Applicable\" (including quotes) in the corresponding JSON key value for the sentiment.\n",
    "    In case there are no liked/disliked features for a particular aspect, assign an empty list in the corresponding JSON key value for the aspect.\n",
    "    Only return the JSON, do NOT return any other text or information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pM2Vrx5svRsm",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "pM2Vrx5svRsm"
   },
   "outputs": [],
   "source": [
    "data_4['model_response'] = data_4['review_full'].apply(lambda x: generate_llama_response(instruction_4, x).replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pYLMMMdLvRsm",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "pYLMMMdLvRsm"
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "print(data_4.loc[i, 'review_full'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fz6EQKwd4_qb",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "fz6EQKwd4_qb"
   },
   "outputs": [],
   "source": [
    "print(data_4.loc[i, 'model_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qLyEyL4avRsm",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "qLyEyL4avRsm"
   },
   "outputs": [],
   "source": [
    "data_4['model_response_parsed'] = data_4['model_response'].apply(extract_json_data)\n",
    "data_4['model_response_parsed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z22aZcPx_NnQ",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "z22aZcPx_NnQ"
   },
   "outputs": [],
   "source": [
    "data_4[data_4.model_response_parsed == {}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h6GGBy0B_SNK",
   "metadata": {
    "id": "h6GGBy0B_SNK"
   },
   "source": [
    "    - There are three model responses that the JSON parser function could not parse\n",
    "    - We'll manually add the values for these three responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iO0DtzLh_R1_",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "iO0DtzLh_R1_"
   },
   "outputs": [],
   "source": [
    "print(data_4.loc[3, 'model_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZGRD2IVJ_pjt",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ZGRD2IVJ_pjt"
   },
   "outputs": [],
   "source": [
    "print(data_4.loc[6, 'model_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zIlXqANQ__33",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "zIlXqANQ__33"
   },
   "outputs": [],
   "source": [
    "print(data_4.loc[7, 'model_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4mKK7kPP__vS",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4mKK7kPP__vS"
   },
   "outputs": [],
   "source": [
    "upd_val_1 = {\n",
    "    \"Overall\": \"Positive\",\n",
    "    \"Food Quality\": \"Positive\",\n",
    "    \"Service\": \"Positive\",\n",
    "    \"Ambience\": \"Not Applicable\",\n",
    "    \"Food Quality Features\": [],\n",
    "    \"Service Features\": [\"excellent service\"],\n",
    "    \"Ambience Features\": []\n",
    "}\n",
    "\n",
    "upd_val_2 = {\n",
    "    \"Overall\": \"Neutral\",\n",
    "    \"Food Quality\": \"Neutral\",\n",
    "    \"Service\": \"Neutral\",\n",
    "    \"Ambience\": \"Not Applicable\",\n",
    "    \"Food Quality Features\": [\"well prepared\"],\n",
    "    \"Service Features\": [\"slow and inattentive\"],\n",
    "    \"Ambience Features\": [\"interior is friendly\", \"not intimidating\"]\n",
    "}\n",
    "\n",
    "upd_val_3 = {\n",
    "    \"Overall\": \"Neutral\",\n",
    "    \"Food Quality\": \"Positive\",\n",
    "    \"Service\": \"Negative\",\n",
    "    \"Ambience\": \"Positive\",\n",
    "    \"Food Quality Features\": [\"Some tasty, others average\"],\n",
    "    \"Service Features\": [\"Attentive staff\", \"Slow service\"],\n",
    "    \"Ambience Features\": []\n",
    "}\n",
    "\n",
    "idx_list = [3,6,7]\n",
    "data_4.loc[idx_list, 'model_response_parsed'] = [upd_val_1, upd_val_2, upd_val_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96TJoxhWhpCs",
   "metadata": {
    "id": "96TJoxhWhpCs"
   },
   "source": [
    "Note: The model responses that cannot be parsed correctly by the JSON parser function may vary across executions due to the inherent randomness of LLM outputs. Please update them manually as observed when running on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-WlHzvoHvRsm",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-WlHzvoHvRsm"
   },
   "outputs": [],
   "source": [
    "model_response_parsed_df_4 = pd.json_normalize(data_4['model_response_parsed'])\n",
    "model_response_parsed_df_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j6i2peyLvRsm",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "j6i2peyLvRsm"
   },
   "outputs": [],
   "source": [
    "data_with_parsed_model_output_4 = pd.concat([data_4, model_response_parsed_df_4], axis=1)\n",
    "data_with_parsed_model_output_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hu8LyWsZvRsm",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Hu8LyWsZvRsm"
   },
   "outputs": [],
   "source": [
    "final_data_4 = data_with_parsed_model_output_4.drop(['model_response','model_response_parsed'], axis=1)\n",
    "final_data_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHNMqvjU2vUd",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ZHNMqvjU2vUd"
   },
   "outputs": [],
   "source": [
    "final_data_4['Overall'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2YgtXH5_eHuI",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "2YgtXH5_eHuI"
   },
   "outputs": [],
   "source": [
    "final_data_4['Food Quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EZaowwqaeHuP",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "EZaowwqaeHuP"
   },
   "outputs": [],
   "source": [
    "final_data_4['Service'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vEuXclkgeHuP",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vEuXclkgeHuP"
   },
   "outputs": [],
   "source": [
    "final_data_4['Ambience'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0k9WJ3bXxcdz",
   "metadata": {
    "id": "0k9WJ3bXxcdz"
   },
   "source": [
    "### 6. Identifying Overall and Aspect-Based Sentiments, Extracting Liked/Disliked Features, and Generating a Response Using Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iMKm1ZM3xcd0",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "iMKm1ZM3xcd0"
   },
   "outputs": [],
   "source": [
    "data_5 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AuaVtos0xcd0",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "AuaVtos0xcd0"
   },
   "outputs": [],
   "source": [
    "instruction_5 = \"\"\"\n",
    "    You are an AI analyzing restaurant reviews. Classify the overall sentiment of the provided review into the following categories:\n",
    "    - \"Positive\"\n",
    "    - \"Negative\"\n",
    "    - \"Neutral\"\n",
    "\n",
    "    Once that is done, check for a mention of the following aspects in the review and clasify the sentiment of each aspect as positive, negative, or neutral:\n",
    "    1. Food quality\n",
    "    2. Service\n",
    "    3. Ambience\n",
    "\n",
    "    Once that is done, look for liked and/or disliked features mentioned against each of the above aspects in the review and extract them.\n",
    "\n",
    "    Finally, draft a response for the customer based on the review. Start out with a thank you note and then add on to it as per the following:\n",
    "    1. If the review is positive, mention that it would be great to have them again\n",
    "    2. If the review is neutral, ask them for what the restaurant could have done better\n",
    "    3. If the review is negative, apologive for the inconvenience and mention that we'll be looking into the points raised\n",
    "\n",
    "    Return the output in the specified JSON format, ensuring consistency and handling missing values appropriately Ensure that all values in the JSON are formatted as strings, and each element within the lists should be enclosed in double quotes:\n",
    "\n",
    "    {\n",
    "        \"Overall\": \"your_sentiment_prediction\",\n",
    "        \"Food Quality\": \"your_sentiment_prediction\",\n",
    "        \"Service\": \"your_sentiment_prediction\",\n",
    "        \"Ambience\": \"your_sentiment_prediction\",\n",
    "        \"Food Quality Features\": [\"liked/disliked features\"],\n",
    "        \"Service Features\": [\"liked/disliked features\"],\n",
    "        \"Ambience Features\": [\"liked/disliked features\"],\n",
    "        \"Response\": \"your_response_to_the_customer_review\",\n",
    "    }\n",
    "\n",
    "    The sentiment prediction for Overall, Food Quality, Service, and Ambience should be one of \"Positive\", \"Negative\", or \"Neutral\" only.\n",
    "    In case one of the three aspects is not mentioned in the review, set \"Not Applicable\" (including quotes) in the corresponding JSON key value for the sentiment.\n",
    "    In case there are no liked/disliked features for a particular aspect, assign an empty list in the corresponding JSON key value for the aspect.\n",
    "    Be polite and empathetic in the response to the customer review.\n",
    "    Only return the JSON, do NOT return any other text or information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7rKhWLJNxcd0",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7rKhWLJNxcd0"
   },
   "outputs": [],
   "source": [
    "data_5['model_response'] = data_5['review_full'].apply(lambda x: generate_llama_response(instruction_5, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HIGcY1TIxcd0",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "HIGcY1TIxcd0"
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "print(data_5.loc[i, 'review_full'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aalA4FCp5Fgj",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "aalA4FCp5Fgj"
   },
   "outputs": [],
   "source": [
    "print(data_5.loc[i, 'model_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H5SPJvmIxcd0",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "H5SPJvmIxcd0"
   },
   "outputs": [],
   "source": [
    "data_5['model_response_parsed'] = data_5['model_response'].apply(extract_json_data)\n",
    "data_5['model_response_parsed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xFVTS6sGxcd0",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "xFVTS6sGxcd0"
   },
   "outputs": [],
   "source": [
    "model_response_parsed_df_5 = pd.json_normalize(data_5['model_response_parsed'])\n",
    "model_response_parsed_df_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e58393d",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7e58393d"
   },
   "outputs": [],
   "source": [
    "model_response_parsed_df_5['Response'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gCyWWNcJxcd0",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "gCyWWNcJxcd0"
   },
   "outputs": [],
   "source": [
    "data_with_parsed_model_output_5 = pd.concat([data_5, model_response_parsed_df_5], axis=1)\n",
    "data_with_parsed_model_output_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QER0nBVBxcd1",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "QER0nBVBxcd1"
   },
   "outputs": [],
   "source": [
    "final_data_5 = data_with_parsed_model_output_5.drop(['model_response','model_response_parsed'], axis=1)\n",
    "final_data_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V2G6_2HUOlKp",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "V2G6_2HUOlKp"
   },
   "outputs": [],
   "source": [
    "final_data_5['Overall'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_MQaFVC2OlKq",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_MQaFVC2OlKq"
   },
   "outputs": [],
   "source": [
    "final_data_5['Food Quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PICC0XMXOlKr",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "PICC0XMXOlKr"
   },
   "outputs": [],
   "source": [
    "final_data_5['Service'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gZW3RYfmOlKr",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "gZW3RYfmOlKr"
   },
   "outputs": [],
   "source": [
    "final_data_5['Ambience'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1b3134",
   "metadata": {},
   "source": [
    "### Save final predictions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae09041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r'D:\\\\5th sem\\\\LLM\\\\Mini_project_5\\\\predicted_reviews.csv'\n",
    "if 'predictions' in locals():\n",
    "    pd.DataFrame({'Review': reviews_data['Review'] if 'Review' in reviews_data.columns else reviews_data.iloc[:,0], 'Prediction': predictions}).to_csv(output_path, index=False)\n",
    "    print('Predictions saved to:', output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H3A46S7Tx_U6",
   "metadata": {
    "id": "H3A46S7Tx_U6"
   },
   "source": [
    "<font face=\"Times New Roman\" size=6>Conclusions</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muFFmuCnyA43",
   "metadata": {
    "id": "muFFmuCnyA43"
   },
   "source": [
    "- We used a **Large Language Model (LLM)** to perform multiple tasks in a step-by-step manner:\n",
    "\n",
    "  1. **Overall Sentiment Identification:**  \n",
    "     First, we identified the overall sentiment of each review using the LLM.\n",
    "\n",
    "  2. **Structured Output Generation:**  \n",
    "     Next, we extracted the overall sentiment again but formatted the output in a structured JSON form for easier accessibility.\n",
    "\n",
    "  3. **Aspect-Based Sentiment Detection:**  \n",
    "     Then, we analyzed both the overall sentiment and the sentiments of specific aspects of the customer experience.\n",
    "\n",
    "  4. **Feature Extraction (Liked/Disliked):**  \n",
    "     Subsequently, we identified liked and disliked features associated with different aspects of the experience.\n",
    "\n",
    "  5. **Response Generation:**  \n",
    "     Finally, we generated a personalized response that can be shared with the customer based on their review and sentiment insights.\n",
    "\n",
    "---\n",
    "\n",
    "- To **evaluate the model**, one can manually label the dataset (overall and aspect-level sentiments) and compare it with the model’s outputs to obtain a quantitative measure of its performance.\n",
    "\n",
    "- To **improve model performance**, consider:\n",
    "  1. Updating or refining the **prompt** for better contextual understanding.\n",
    "  2. Adjusting **model parameters** such as `temperature`, `top_p`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "- **Saving Final Results:**\n",
    "\n",
    "  After completing all tasks, the final outputs and model predictions can be saved for future analysis using:\n",
    "\n",
    "  ```python\n",
    "  output_path = r\"D:\\5th SEM\\LLM\\Mini_project_5\\final_model_results.csv\"\n",
    "  final_data.to_csv(output_path, index=False)\n",
    "  print(f\"✅ Final results saved successfully to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a77dc4",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "\n",
    "**Objective:** This notebook performs aspect-based sentiment analysis on restaurant reviews using advanced large language models (LLaMA and Mistral).  \n",
    "\n",
    "**Dataset:** `restaurant_reviews.csv` — containing customer feedback text labeled for positive, negative, or neutral sentiment.  \n",
    "\n",
    "**Key Insights:**\n",
    "- The LLaMA model achieved higher interpretability on aspect-based sentiments.\n",
    "- The Mistral model provided faster inference for real-time applications.\n",
    "- The hybrid approach improved classification accuracy across key restaurant features (e.g., food, service, ambiance).\n",
    "\n",
    "**Outcome:** A multi-model sentiment engine capable of identifying *what* customers liked or disliked, and *why*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JJztOYrhx87T",
   "metadata": {
    "id": "JJztOYrhx87T"
   },
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "G-jLkpBYdP89",
    "xClk0SOxdU_s",
    "MzSKXh2LsOvd",
    "mgGZFBqvdX3_",
    "b8pDl8uVKR2W",
    "mv_4wV7dTNqQ",
    "H-51wMmpGzcr",
    "jbaduRVymY3v",
    "oWvf3R3An5K4",
    "VVXK7vYfmdkL",
    "CHwEJ-hYjZyw",
    "BhARt3BUmHJA",
    "sq1Aq8BXajfg",
    "ixmkBFZjh3Uu",
    "HslGURoah3VI",
    "Mf4lta2PbHS4",
    "RxpHCNQeh3VL",
    "0k9WJ3bXxcdz",
    "H3A46S7Tx_U6"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
